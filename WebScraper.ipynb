{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16612d15-dc26-4f28-9600-61c4f62d1f83",
   "metadata": {},
   "source": [
    "# WebScraper and Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cd26e-1690-4533-acc6-4549d67a0a68",
   "metadata": {},
   "source": [
    "## Install Dependicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bcf4e-83a9-4cb2-a232-e322036f83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import PyPDF2\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Path to the CSV file\n",
    "output_file_path = \"/data/output.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(output_file_path)\n",
    "\n",
    "# Ensure the CSV file has a column named 'text'\n",
    "if 'text' not in df.columns:\n",
    "    raise ValueError(\"CSV file must have a 'text' column\")\n",
    "\n",
    "# Extract sentences from the CSV file\n",
    "sentences = df['text'].tolist()\n",
    "\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose any model you prefer\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embeddings = model.encode(sentences).astype('float32')\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)  # Add embeddings to the index\n",
    "\n",
    "# Generate embedding for your query sentence\n",
    "query_sentence = \"What is a dialogue dataset?\"\n",
    "query_embedding = model.encode(query_sentence).astype('float32').reshape(1, -1)  # Reshape to 2D array\n",
    "\n",
    "# Number of nearest neighbors to search for\n",
    "k = 3  # You can change this to any number you want\n",
    "\n",
    "# Search the index\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Display the results\n",
    "print(\"Query Sentence:\", query_sentence)\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Distances to nearest neighbors:\", distances)\n",
    "\n",
    "# Retrieve and print the most similar sentences\n",
    "for i in range(k):\n",
    "    print(f\"Similar Sentence {i + 1}: {sentences[indices[0][i]]} (Distance: {distances[0][i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbfd0a-9ca2-4a95-8d86-b1e087868c43",
   "metadata": {},
   "source": [
    "## Download PDFs from single webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8c54b3-db09-412e-8260-1490b268dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1 PDF(s) into the 'pdfs' folder.\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for PDFs if it doesn't exist\n",
    "pdf_dir = 'pdfs'\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "url = 'https://paperswithcode.com/paper/learning-to-memorize-entailment-and-discourse'  # Replace with the target URL\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "pdf_links = []\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].endswith('.pdf'):\n",
    "        pdf_links.append(link['href'])\n",
    "\n",
    "for pdf_link in pdf_links:\n",
    "    # If the link is relative, make it absolute\n",
    "    if not pdf_link.startswith('http'):\n",
    "        pdf_link = url + pdf_link  # Adjust this based on the website structure\n",
    "\n",
    "    pdf_response = requests.get(pdf_link)\n",
    "    pdf_name = pdf_link.split('/')[-1]  # Get the file name from the URL\n",
    "\n",
    "    # Save the PDF in the 'pdfs' directory\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_name)\n",
    "\n",
    "    with open(pdf_path, 'wb') as pdf_file:\n",
    "        pdf_file.write(pdf_response.content)\n",
    "\n",
    "print(f\"Downloaded {len(pdf_links)} PDF(s) into the '{pdf_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140bb89-4770-49a2-8ad6-63631dbc6af1",
   "metadata": {},
   "source": [
    "## Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d559e0-a832-4be7-bd5f-fef0aebe27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(documents):\n",
    "    \"\"\"Splits documents into smaller pieces for processing\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Maximum size of each text chunk\n",
    "        chunk_overlap=300,      # Overlap between chunks for context continuity\n",
    "        is_separator_regex=False  # Disable regex-based splitting\n",
    "    )\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e020c1-596f-44f5-898a-c748b3f49e36",
   "metadata": {},
   "source": [
    "## Extract Text From PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2465f9c1-305f-4474-8df3-99d60b0cdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"  # Add a newline for separation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6cc49-10d6-4729-960b-abb8236d564a",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9897b64-23cb-4382-ac64-e7f4a562c9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "page_content='Learning to Memorize Entailment and Discourse Relations for Persona-Consistent\n",
      "Dialogues\n",
      "Ruijun Chen1, Jin Wang1*, Liang-Chih Yu2and Xuejie Zhang1\n",
      "1School of Information Science and Engineering, Yunnan University, Yunnan, China\n",
      "2Department of Information Management, Yuan Ze University, Taiwan\n",
      "chenrj@mail.ynu.edu.cn, wangjin@ynu.edu.cn, lcyu@saturn.yzu.edu.tw, xjzhang@ynu.edu.cn\n",
      "Abstract\n",
      "Maintaining engagement and consistency is particularly im-\n",
      "portant in dialogue systems. Existing works have improved\n",
      "the performance of dialogue systems by intentionally learn-\n",
      "ing interlocutor personas with sophisticated network struc-\n",
      "tures. One issue with this approach is that it requires more\n",
      "personal corpora with annotations. Additionally, these mod-\n",
      "els typically perform the next utterance prediction to gener-\n",
      "ate a response but neglect the discourse coherence in the en-\n",
      "tire conversation. To address these issues, this study proposes\n",
      "a method of learning to memorize entailment and discourse' metadata={'filename': '2301.04871v2.pdf'}\n",
      "\n",
      "Chunk 2:\n",
      "page_content='els typically perform the next utterance prediction to gener-\n",
      "ate a response but neglect the discourse coherence in the en-\n",
      "tire conversation. To address these issues, this study proposes\n",
      "a method of learning to memorize entailment and discourse\n",
      "relations for persona-consistent dialogue tasks. Entailment\n",
      "text pairs in natural language inference dataset were applied\n",
      "to learn latent entailment relations as external memories by\n",
      "premise-to-hypothesis generation task. Furthermore, an in-\n",
      "ternal memory with a similar architecture was applied to the\n",
      "discourse information in the dialogue. Placing orthogonality\n",
      "restrictions on these two memory spaces ensures that the la-\n",
      "tent entailment relations remain dialogue-independent. Both\n",
      "memories collaborate to obtain entailment and discourse\n",
      "representation for the generation, allowing a deeper under-\n",
      "standing of both consistency and coherence. Experiments on\n",
      "two large public datasets, PersonaChat and DSTC7-A VSD,' metadata={'filename': '2301.04871v2.pdf'}\n",
      "\n",
      "Chunk 3:\n",
      "page_content='tent entailment relations remain dialogue-independent. Both\n",
      "memories collaborate to obtain entailment and discourse\n",
      "representation for the generation, allowing a deeper under-\n",
      "standing of both consistency and coherence. Experiments on\n",
      "two large public datasets, PersonaChat and DSTC7-A VSD,\n",
      "demonstrated the effectiveness of the proposed method. Both\n",
      "automatic and human evaluations indicate that the proposed\n",
      "model outperforms several strong baselines in terms of both\n",
      "persona consistency and response coherence. Our source\n",
      "code is available at https://github.com/Chenrj233/LMEDR.\n",
      "Introduction\n",
      "Traditional chit-chat models lack speciﬁcity and personal-\n",
      "ity consistency. Only when they access a sufﬁciently large\n",
      "dataset will they have the opportunity to generate piecemeal\n",
      "and uninformative responses in a chit-chat setting. For two\n",
      "consecutive questions with similar meanings in a two-round\n",
      "dialogue, that is, what is your job andwhat do you do , the' metadata={'filename': '2301.04871v2.pdf'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the folder containing PDF files\n",
    "pdf_folder_path = \"pdfs\"  # The folder containing your PDF files\n",
    "\n",
    "# List to hold all extracted text as Document objects\n",
    "all_documents = []\n",
    "\n",
    "# Iterate through all PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Create a Document object\n",
    "        document = Document(page_content=pdf_text, metadata={\"filename\": filename})\n",
    "        all_documents.append(document)\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_chunks = split_into_chunks(all_documents)\n",
    "\n",
    "# Print the first few chunks\n",
    "for i, chunk in enumerate(text_chunks[:3]):  # Print first 3 chunks\n",
    "    print(f\"Chunk {i + 1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5d7e1-e3cd-4460-b623-e6b9b52a265e",
   "metadata": {},
   "source": [
    "## Setting Up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa857d6d-b220-4a46-8a8f-da3af1c3eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_model():\n",
    "    \"\"\"Creates the text embedding model\"\"\"\n",
    "    # Initialize and return a HuggingFace embeddings model\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc33566f-57e9-418d-b34a-7d6bba0e3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_database(database_path):\n",
    "    \"\"\"Sets up the vector database connection\"\"\"\n",
    "    # Create the parent directory for the database if it doesn't exist\n",
    "    os.makedirs(os.path.split(database_path)[0], exist_ok=True)\n",
    "    \n",
    "    # Connect to the Milvus database using the provided path\n",
    "    connections.connect(\n",
    "        alias=\"default\",        # Use the default connection alias\n",
    "        uri=database_path       # Specify the database location\n",
    "    )\n",
    "    \n",
    "    # Check if the collection exists\n",
    "    if not utility.has_collection(\"PapersWithCode\"):\n",
    "        print(\"Collection does not exist. You'll need to create it later.\")\n",
    "        return False\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
